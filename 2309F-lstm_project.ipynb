{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"\"\"\n",
    "Each word in the vocabulary is represented as a one-hot vector. This creates a sparse matrix but ensures that words are uniquely identifiable.\n",
    "Embedding layers convert sparse one-hot vectors into dense, low-dimensional vectors capturing semantic meanings of words. This reduces dimensionality and improves training speed.\n",
    "Preprocessing the text—removing punctuation, converting to lowercase, and handling stop words—ensures consistency in data.\n",
    "The dataset is split into training, validation, and test sets to evaluate the model's performance and prevent overfitting.\n",
    "Since LSTMs require sequences of equal length, shorter sequences are padded with zeros or truncated to fit.\n",
    "Some words may occur more frequently than others. Using techniques like class weighting or sub-sampling balances the dataset.\n",
    "Analyzing text statistics like word frequency, sequence lengths, and vocabulary distribution helps understand the dataset better.\n",
    "Normalizing input sequences (e.g., dividing token values by max token value) ensures consistent scaling.\n",
    "Use undersampling or oversampling if certain words are underrepresented or overrepresented.\n",
    "The model typically uses categorical_crossentropy as the loss function for multi-class classification (predicting one word out of many).\n",
    "Accuracy or perplexity can be used to measure the model's performance.\n",
    "Unknown words are often replaced with a special <OOV> token to handle unseen data during training.\n",
    "To predict the next word, a fixed-size context window (e.g., the last 5 words) is used as input to the LSTM.\n",
    "Longer sequences capture more context but increase computational cost, requiring careful selection.\n",
    "Synonym replacement, shuffling, or paraphrasing increases training data diversity.\n",
    "Words with low frequency can be oversampled to improve prediction accuracy for rare words.\n",
    "Dropout helps prevent overfitting in LSTMs during training by randomly deactivating some neurons.\n",
    "Data is split into mini-batches for faster and more efficient training.\n",
    "Shuffling sequences during training prevents the model from memorizing patterns in the order of the data.\n",
    "Sequences are fed into the LSTM step-by-step, preserving temporal relationships between words.\n",
    "Sequences longer than a predefined limit are truncated to fit memory constraints.\n",
    "Techniques like random insertion, swapping, or deletion of words introduce variability.\n",
    "Adjusting the window size allows fine-tuning the level of context captured by the model.\n",
    "Using embeddings like Word2Vec or GloVe helps the model start with semantic knowledge.\n",
    "Optimizers like Adam or RMSprop adjust weights efficiently during backpropagation.\n",
    "Statistical analysis of token frequencies can inform vocabulary curation.\n",
    "Rare word counts are scaled logarithmically to reduce their impact.\n",
    "Analyzing word distributions helps identify and mitigate potential biases.\n",
    "Overlapping sequences ensure every word is part of multiple training sequences.\n",
    "The choice of embedding dimension (e.g., 100, 300) balances richness and computational cost.\n",
    "Hidden layers capture dependencies in sequences, improving predictions for longer contexts.\n",
    "Prevents exploding gradients during backpropagation.\n",
    "Training is halted once validation loss stops improving, preventing overfitting.\n",
    "Resampling ensures the target word's frequency matches its importance in the text.\n",
    "Adjusting learning rates during training improves convergence.\n",
    "Different languages require custom preprocessing (e.g., stemming or lemmatization).\n",
    "Incorporating positional encodings can enhance the model's ability to capture order.\n",
    "Multi-layer LSTMs capture both local and global context.\n",
    "Adding noise to input data prevents overfitting and improves generalization.\n",
    "Overlap percentages in sliding windows can be tuned for sequence diversity.\n",
    "Compute statistics like mean, median, and variance of sequence lengths.\n",
    "Adding metadata like POS tags or sentence boundaries can enhance model performance.\n",
    "Pretrain the model on unsupervised tasks like masked word prediction to improve accuracy.\n",
    "Metrics like perplexity, BLEU, or ROUGE assess next-word prediction performance.\n",
    "Iteratively train on subsets of data to improve learning on difficult patterns.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "J1D42emD32Ro"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KhtDxwL_AXFj"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "K8MRFre9AaG9"
   },
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YrpAl3EDAgvh",
    "outputId": "cabe3c83-7274-4512-94d5-b5def2b72453"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'to': 2,\n",
       " 'or': 3,\n",
       " 'of': 4,\n",
       " 'and': 5,\n",
       " 'words': 6,\n",
       " 'training': 7,\n",
       " 'sequences': 8,\n",
       " 'word': 9,\n",
       " 'like': 10,\n",
       " 'in': 11,\n",
       " 'are': 12,\n",
       " 'data': 13,\n",
       " 'is': 14,\n",
       " 'model': 15,\n",
       " 'can': 16,\n",
       " 'during': 17,\n",
       " 'a': 18,\n",
       " 'for': 19,\n",
       " 'into': 20,\n",
       " 'performance': 21,\n",
       " 'overfitting': 22,\n",
       " 'with': 23,\n",
       " 'helps': 24,\n",
       " 'e': 25,\n",
       " 'g': 26,\n",
       " 'token': 27,\n",
       " 'by': 28,\n",
       " 'context': 29,\n",
       " 'capture': 30,\n",
       " 'vocabulary': 31,\n",
       " 'as': 32,\n",
       " 'one': 33,\n",
       " 'ensures': 34,\n",
       " 'improves': 35,\n",
       " 'dataset': 36,\n",
       " \"model's\": 37,\n",
       " 'lstms': 38,\n",
       " 'more': 39,\n",
       " 'frequency': 40,\n",
       " 'sequence': 41,\n",
       " 'input': 42,\n",
       " 'accuracy': 43,\n",
       " 'be': 44,\n",
       " 'longer': 45,\n",
       " 'improve': 46,\n",
       " 'prediction': 47,\n",
       " 'prevents': 48,\n",
       " 'on': 49,\n",
       " 'hot': 50,\n",
       " 'this': 51,\n",
       " 'sparse': 52,\n",
       " 'but': 53,\n",
       " 'embedding': 54,\n",
       " 'layers': 55,\n",
       " 'vectors': 56,\n",
       " 'low': 57,\n",
       " 'semantic': 58,\n",
       " 'preprocessing': 59,\n",
       " 'split': 60,\n",
       " 'validation': 61,\n",
       " 'prevent': 62,\n",
       " 'require': 63,\n",
       " 'truncated': 64,\n",
       " 'fit': 65,\n",
       " 'some': 66,\n",
       " 'than': 67,\n",
       " 'using': 68,\n",
       " 'techniques': 69,\n",
       " 'class': 70,\n",
       " 'balances': 71,\n",
       " 'analyzing': 72,\n",
       " 'text': 73,\n",
       " 'statistics': 74,\n",
       " 'lengths': 75,\n",
       " 'loss': 76,\n",
       " 'multi': 77,\n",
       " 'perplexity': 78,\n",
       " 'used': 79,\n",
       " 'next': 80,\n",
       " 'size': 81,\n",
       " 'window': 82,\n",
       " 'lstm': 83,\n",
       " 'computational': 84,\n",
       " 'cost': 85,\n",
       " 'shuffling': 86,\n",
       " 'diversity': 87,\n",
       " 'rare': 88,\n",
       " 'patterns': 89,\n",
       " 'order': 90,\n",
       " 'step': 91,\n",
       " 'adjusting': 92,\n",
       " 'backpropagation': 93,\n",
       " 'improving': 94,\n",
       " 'learning': 95,\n",
       " 'enhance': 96,\n",
       " 'adding': 97,\n",
       " 'each': 98,\n",
       " 'represented': 99,\n",
       " 'vector': 100,\n",
       " 'creates': 101,\n",
       " 'matrix': 102,\n",
       " 'that': 103,\n",
       " 'uniquely': 104,\n",
       " 'identifiable': 105,\n",
       " 'convert': 106,\n",
       " 'dense': 107,\n",
       " 'dimensional': 108,\n",
       " 'capturing': 109,\n",
       " 'meanings': 110,\n",
       " 'reduces': 111,\n",
       " 'dimensionality': 112,\n",
       " 'speed': 113,\n",
       " 'text—removing': 114,\n",
       " 'punctuation': 115,\n",
       " 'converting': 116,\n",
       " 'lowercase': 117,\n",
       " 'handling': 118,\n",
       " 'stop': 119,\n",
       " 'words—ensures': 120,\n",
       " 'consistency': 121,\n",
       " 'test': 122,\n",
       " 'sets': 123,\n",
       " 'evaluate': 124,\n",
       " 'since': 125,\n",
       " 'equal': 126,\n",
       " 'length': 127,\n",
       " 'shorter': 128,\n",
       " 'padded': 129,\n",
       " 'zeros': 130,\n",
       " 'may': 131,\n",
       " 'occur': 132,\n",
       " 'frequently': 133,\n",
       " 'others': 134,\n",
       " 'weighting': 135,\n",
       " 'sub': 136,\n",
       " 'sampling': 137,\n",
       " 'distribution': 138,\n",
       " 'understand': 139,\n",
       " 'better': 140,\n",
       " 'normalizing': 141,\n",
       " 'dividing': 142,\n",
       " 'values': 143,\n",
       " 'max': 144,\n",
       " 'value': 145,\n",
       " 'consistent': 146,\n",
       " 'scaling': 147,\n",
       " 'use': 148,\n",
       " 'undersampling': 149,\n",
       " 'oversampling': 150,\n",
       " 'if': 151,\n",
       " 'certain': 152,\n",
       " 'underrepresented': 153,\n",
       " 'overrepresented': 154,\n",
       " 'typically': 155,\n",
       " 'uses': 156,\n",
       " 'categorical': 157,\n",
       " 'crossentropy': 158,\n",
       " 'function': 159,\n",
       " 'classification': 160,\n",
       " 'predicting': 161,\n",
       " 'out': 162,\n",
       " 'many': 163,\n",
       " 'measure': 164,\n",
       " 'unknown': 165,\n",
       " 'often': 166,\n",
       " 'replaced': 167,\n",
       " 'special': 168,\n",
       " 'oov': 169,\n",
       " 'handle': 170,\n",
       " 'unseen': 171,\n",
       " 'predict': 172,\n",
       " 'fixed': 173,\n",
       " 'last': 174,\n",
       " '5': 175,\n",
       " 'increase': 176,\n",
       " 'requiring': 177,\n",
       " 'careful': 178,\n",
       " 'selection': 179,\n",
       " 'synonym': 180,\n",
       " 'replacement': 181,\n",
       " 'paraphrasing': 182,\n",
       " 'increases': 183,\n",
       " 'oversampled': 184,\n",
       " 'dropout': 185,\n",
       " 'randomly': 186,\n",
       " 'deactivating': 187,\n",
       " 'neurons': 188,\n",
       " 'mini': 189,\n",
       " 'batches': 190,\n",
       " 'faster': 191,\n",
       " 'efficient': 192,\n",
       " 'from': 193,\n",
       " 'memorizing': 194,\n",
       " 'fed': 195,\n",
       " 'preserving': 196,\n",
       " 'temporal': 197,\n",
       " 'relationships': 198,\n",
       " 'between': 199,\n",
       " 'predefined': 200,\n",
       " 'limit': 201,\n",
       " 'memory': 202,\n",
       " 'constraints': 203,\n",
       " 'random': 204,\n",
       " 'insertion': 205,\n",
       " 'swapping': 206,\n",
       " 'deletion': 207,\n",
       " 'introduce': 208,\n",
       " 'variability': 209,\n",
       " 'allows': 210,\n",
       " 'fine': 211,\n",
       " 'tuning': 212,\n",
       " 'level': 213,\n",
       " 'captured': 214,\n",
       " 'embeddings': 215,\n",
       " 'word2vec': 216,\n",
       " 'glove': 217,\n",
       " 'start': 218,\n",
       " 'knowledge': 219,\n",
       " 'optimizers': 220,\n",
       " 'adam': 221,\n",
       " 'rmsprop': 222,\n",
       " 'adjust': 223,\n",
       " 'weights': 224,\n",
       " 'efficiently': 225,\n",
       " 'statistical': 226,\n",
       " 'analysis': 227,\n",
       " 'frequencies': 228,\n",
       " 'inform': 229,\n",
       " 'curation': 230,\n",
       " 'counts': 231,\n",
       " 'scaled': 232,\n",
       " 'logarithmically': 233,\n",
       " 'reduce': 234,\n",
       " 'their': 235,\n",
       " 'impact': 236,\n",
       " 'distributions': 237,\n",
       " 'identify': 238,\n",
       " 'mitigate': 239,\n",
       " 'potential': 240,\n",
       " 'biases': 241,\n",
       " 'overlapping': 242,\n",
       " 'ensure': 243,\n",
       " 'every': 244,\n",
       " 'part': 245,\n",
       " 'multiple': 246,\n",
       " 'choice': 247,\n",
       " 'dimension': 248,\n",
       " '100': 249,\n",
       " '300': 250,\n",
       " 'richness': 251,\n",
       " 'hidden': 252,\n",
       " 'dependencies': 253,\n",
       " 'predictions': 254,\n",
       " 'contexts': 255,\n",
       " 'exploding': 256,\n",
       " 'gradients': 257,\n",
       " 'halted': 258,\n",
       " 'once': 259,\n",
       " 'stops': 260,\n",
       " 'preventing': 261,\n",
       " 'resampling': 262,\n",
       " 'target': 263,\n",
       " \"word's\": 264,\n",
       " 'matches': 265,\n",
       " 'its': 266,\n",
       " 'importance': 267,\n",
       " 'rates': 268,\n",
       " 'convergence': 269,\n",
       " 'different': 270,\n",
       " 'languages': 271,\n",
       " 'custom': 272,\n",
       " 'stemming': 273,\n",
       " 'lemmatization': 274,\n",
       " 'incorporating': 275,\n",
       " 'positional': 276,\n",
       " 'encodings': 277,\n",
       " 'ability': 278,\n",
       " 'layer': 279,\n",
       " 'both': 280,\n",
       " 'local': 281,\n",
       " 'global': 282,\n",
       " 'noise': 283,\n",
       " 'generalization': 284,\n",
       " 'overlap': 285,\n",
       " 'percentages': 286,\n",
       " 'sliding': 287,\n",
       " 'windows': 288,\n",
       " 'tuned': 289,\n",
       " 'compute': 290,\n",
       " 'mean': 291,\n",
       " 'median': 292,\n",
       " 'variance': 293,\n",
       " 'metadata': 294,\n",
       " 'pos': 295,\n",
       " 'tags': 296,\n",
       " 'sentence': 297,\n",
       " 'boundaries': 298,\n",
       " 'pretrain': 299,\n",
       " 'unsupervised': 300,\n",
       " 'tasks': 301,\n",
       " 'masked': 302,\n",
       " 'metrics': 303,\n",
       " 'bleu': 304,\n",
       " 'rouge': 305,\n",
       " 'assess': 306,\n",
       " 'iteratively': 307,\n",
       " 'train': 308,\n",
       " 'subsets': 309,\n",
       " 'difficult': 310}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "44VahqKdAjr9"
   },
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for sentence in doc.split('\\n'):\n",
    "    tokenized_sentence = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    #print(tokenized_sentence)\n",
    "    for i in range(1,len(tokenized_sentence)):\n",
    "        input_sequences.append(tokenized_sentence[:i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UyqwPDzNA5mR",
    "outputId": "98e99353-46c5-4252-c280-85f8eeef1dec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[98, 9],\n",
       " [98, 9, 11],\n",
       " [98, 9, 11, 1],\n",
       " [98, 9, 11, 1, 31],\n",
       " [98, 9, 11, 1, 31, 14],\n",
       " [98, 9, 11, 1, 31, 14, 99],\n",
       " [98, 9, 11, 1, 31, 14, 99, 32],\n",
       " [98, 9, 11, 1, 31, 14, 99, 32, 18],\n",
       " [98, 9, 11, 1, 31, 14, 99, 32, 18, 33],\n",
       " [98, 9, 11, 1, 31, 14, 99, 32, 18, 33, 50],\n",
       " [98, 9, 11, 1, 31, 14, 99, 32, 18, 33, 50, 100],\n",
       " [98, 9, 11, 1, 31, 14, 99, 32, 18, 33, 50, 100, 51],\n",
       " [98, 9, 11, 1, 31, 14, 99, 32, 18, 33, 50, 100, 51, 101],\n",
       " [98, 9, 11, 1, 31, 14, 99, 32, 18, 33, 50, 100, 51, 101, 18],\n",
       " [98, 9, 11, 1, 31, 14, 99, 32, 18, 33, 50, 100, 51, 101, 18, 52],\n",
       " [98, 9, 11, 1, 31, 14, 99, 32, 18, 33, 50, 100, 51, 101, 18, 52, 102],\n",
       " [98, 9, 11, 1, 31, 14, 99, 32, 18, 33, 50, 100, 51, 101, 18, 52, 102, 53],\n",
       " [98, 9, 11, 1, 31, 14, 99, 32, 18, 33, 50, 100, 51, 101, 18, 52, 102, 53, 34],\n",
       " [98,\n",
       "  9,\n",
       "  11,\n",
       "  1,\n",
       "  31,\n",
       "  14,\n",
       "  99,\n",
       "  32,\n",
       "  18,\n",
       "  33,\n",
       "  50,\n",
       "  100,\n",
       "  51,\n",
       "  101,\n",
       "  18,\n",
       "  52,\n",
       "  102,\n",
       "  53,\n",
       "  34,\n",
       "  103],\n",
       " [98,\n",
       "  9,\n",
       "  11,\n",
       "  1,\n",
       "  31,\n",
       "  14,\n",
       "  99,\n",
       "  32,\n",
       "  18,\n",
       "  33,\n",
       "  50,\n",
       "  100,\n",
       "  51,\n",
       "  101,\n",
       "  18,\n",
       "  52,\n",
       "  102,\n",
       "  53,\n",
       "  34,\n",
       "  103,\n",
       "  6],\n",
       " [98,\n",
       "  9,\n",
       "  11,\n",
       "  1,\n",
       "  31,\n",
       "  14,\n",
       "  99,\n",
       "  32,\n",
       "  18,\n",
       "  33,\n",
       "  50,\n",
       "  100,\n",
       "  51,\n",
       "  101,\n",
       "  18,\n",
       "  52,\n",
       "  102,\n",
       "  53,\n",
       "  34,\n",
       "  103,\n",
       "  6,\n",
       "  12],\n",
       " [98,\n",
       "  9,\n",
       "  11,\n",
       "  1,\n",
       "  31,\n",
       "  14,\n",
       "  99,\n",
       "  32,\n",
       "  18,\n",
       "  33,\n",
       "  50,\n",
       "  100,\n",
       "  51,\n",
       "  101,\n",
       "  18,\n",
       "  52,\n",
       "  102,\n",
       "  53,\n",
       "  34,\n",
       "  103,\n",
       "  6,\n",
       "  12,\n",
       "  104],\n",
       " [98,\n",
       "  9,\n",
       "  11,\n",
       "  1,\n",
       "  31,\n",
       "  14,\n",
       "  99,\n",
       "  32,\n",
       "  18,\n",
       "  33,\n",
       "  50,\n",
       "  100,\n",
       "  51,\n",
       "  101,\n",
       "  18,\n",
       "  52,\n",
       "  102,\n",
       "  53,\n",
       "  34,\n",
       "  103,\n",
       "  6,\n",
       "  12,\n",
       "  104,\n",
       "  105],\n",
       " [54, 55],\n",
       " [54, 55, 106],\n",
       " [54, 55, 106, 52],\n",
       " [54, 55, 106, 52, 33],\n",
       " [54, 55, 106, 52, 33, 50],\n",
       " [54, 55, 106, 52, 33, 50, 56],\n",
       " [54, 55, 106, 52, 33, 50, 56, 20],\n",
       " [54, 55, 106, 52, 33, 50, 56, 20, 107],\n",
       " [54, 55, 106, 52, 33, 50, 56, 20, 107, 57],\n",
       " [54, 55, 106, 52, 33, 50, 56, 20, 107, 57, 108],\n",
       " [54, 55, 106, 52, 33, 50, 56, 20, 107, 57, 108, 56],\n",
       " [54, 55, 106, 52, 33, 50, 56, 20, 107, 57, 108, 56, 109],\n",
       " [54, 55, 106, 52, 33, 50, 56, 20, 107, 57, 108, 56, 109, 58],\n",
       " [54, 55, 106, 52, 33, 50, 56, 20, 107, 57, 108, 56, 109, 58, 110],\n",
       " [54, 55, 106, 52, 33, 50, 56, 20, 107, 57, 108, 56, 109, 58, 110, 4],\n",
       " [54, 55, 106, 52, 33, 50, 56, 20, 107, 57, 108, 56, 109, 58, 110, 4, 6],\n",
       " [54, 55, 106, 52, 33, 50, 56, 20, 107, 57, 108, 56, 109, 58, 110, 4, 6, 51],\n",
       " [54,\n",
       "  55,\n",
       "  106,\n",
       "  52,\n",
       "  33,\n",
       "  50,\n",
       "  56,\n",
       "  20,\n",
       "  107,\n",
       "  57,\n",
       "  108,\n",
       "  56,\n",
       "  109,\n",
       "  58,\n",
       "  110,\n",
       "  4,\n",
       "  6,\n",
       "  51,\n",
       "  111],\n",
       " [54,\n",
       "  55,\n",
       "  106,\n",
       "  52,\n",
       "  33,\n",
       "  50,\n",
       "  56,\n",
       "  20,\n",
       "  107,\n",
       "  57,\n",
       "  108,\n",
       "  56,\n",
       "  109,\n",
       "  58,\n",
       "  110,\n",
       "  4,\n",
       "  6,\n",
       "  51,\n",
       "  111,\n",
       "  112],\n",
       " [54,\n",
       "  55,\n",
       "  106,\n",
       "  52,\n",
       "  33,\n",
       "  50,\n",
       "  56,\n",
       "  20,\n",
       "  107,\n",
       "  57,\n",
       "  108,\n",
       "  56,\n",
       "  109,\n",
       "  58,\n",
       "  110,\n",
       "  4,\n",
       "  6,\n",
       "  51,\n",
       "  111,\n",
       "  112,\n",
       "  5],\n",
       " [54,\n",
       "  55,\n",
       "  106,\n",
       "  52,\n",
       "  33,\n",
       "  50,\n",
       "  56,\n",
       "  20,\n",
       "  107,\n",
       "  57,\n",
       "  108,\n",
       "  56,\n",
       "  109,\n",
       "  58,\n",
       "  110,\n",
       "  4,\n",
       "  6,\n",
       "  51,\n",
       "  111,\n",
       "  112,\n",
       "  5,\n",
       "  35],\n",
       " [54,\n",
       "  55,\n",
       "  106,\n",
       "  52,\n",
       "  33,\n",
       "  50,\n",
       "  56,\n",
       "  20,\n",
       "  107,\n",
       "  57,\n",
       "  108,\n",
       "  56,\n",
       "  109,\n",
       "  58,\n",
       "  110,\n",
       "  4,\n",
       "  6,\n",
       "  51,\n",
       "  111,\n",
       "  112,\n",
       "  5,\n",
       "  35,\n",
       "  7],\n",
       " [54,\n",
       "  55,\n",
       "  106,\n",
       "  52,\n",
       "  33,\n",
       "  50,\n",
       "  56,\n",
       "  20,\n",
       "  107,\n",
       "  57,\n",
       "  108,\n",
       "  56,\n",
       "  109,\n",
       "  58,\n",
       "  110,\n",
       "  4,\n",
       "  6,\n",
       "  51,\n",
       "  111,\n",
       "  112,\n",
       "  5,\n",
       "  35,\n",
       "  7,\n",
       "  113],\n",
       " [59, 1],\n",
       " [59, 1, 114],\n",
       " [59, 1, 114, 115],\n",
       " [59, 1, 114, 115, 116],\n",
       " [59, 1, 114, 115, 116, 2],\n",
       " [59, 1, 114, 115, 116, 2, 117],\n",
       " [59, 1, 114, 115, 116, 2, 117, 5],\n",
       " [59, 1, 114, 115, 116, 2, 117, 5, 118],\n",
       " [59, 1, 114, 115, 116, 2, 117, 5, 118, 119],\n",
       " [59, 1, 114, 115, 116, 2, 117, 5, 118, 119, 120],\n",
       " [59, 1, 114, 115, 116, 2, 117, 5, 118, 119, 120, 121],\n",
       " [59, 1, 114, 115, 116, 2, 117, 5, 118, 119, 120, 121, 11],\n",
       " [59, 1, 114, 115, 116, 2, 117, 5, 118, 119, 120, 121, 11, 13],\n",
       " [1, 36],\n",
       " [1, 36, 14],\n",
       " [1, 36, 14, 60],\n",
       " [1, 36, 14, 60, 20],\n",
       " [1, 36, 14, 60, 20, 7],\n",
       " [1, 36, 14, 60, 20, 7, 61],\n",
       " [1, 36, 14, 60, 20, 7, 61, 5],\n",
       " [1, 36, 14, 60, 20, 7, 61, 5, 122],\n",
       " [1, 36, 14, 60, 20, 7, 61, 5, 122, 123],\n",
       " [1, 36, 14, 60, 20, 7, 61, 5, 122, 123, 2],\n",
       " [1, 36, 14, 60, 20, 7, 61, 5, 122, 123, 2, 124],\n",
       " [1, 36, 14, 60, 20, 7, 61, 5, 122, 123, 2, 124, 1],\n",
       " [1, 36, 14, 60, 20, 7, 61, 5, 122, 123, 2, 124, 1, 37],\n",
       " [1, 36, 14, 60, 20, 7, 61, 5, 122, 123, 2, 124, 1, 37, 21],\n",
       " [1, 36, 14, 60, 20, 7, 61, 5, 122, 123, 2, 124, 1, 37, 21, 5],\n",
       " [1, 36, 14, 60, 20, 7, 61, 5, 122, 123, 2, 124, 1, 37, 21, 5, 62],\n",
       " [1, 36, 14, 60, 20, 7, 61, 5, 122, 123, 2, 124, 1, 37, 21, 5, 62, 22],\n",
       " [125, 38],\n",
       " [125, 38, 63],\n",
       " [125, 38, 63, 8],\n",
       " [125, 38, 63, 8, 4],\n",
       " [125, 38, 63, 8, 4, 126],\n",
       " [125, 38, 63, 8, 4, 126, 127],\n",
       " [125, 38, 63, 8, 4, 126, 127, 128],\n",
       " [125, 38, 63, 8, 4, 126, 127, 128, 8],\n",
       " [125, 38, 63, 8, 4, 126, 127, 128, 8, 12],\n",
       " [125, 38, 63, 8, 4, 126, 127, 128, 8, 12, 129],\n",
       " [125, 38, 63, 8, 4, 126, 127, 128, 8, 12, 129, 23],\n",
       " [125, 38, 63, 8, 4, 126, 127, 128, 8, 12, 129, 23, 130],\n",
       " [125, 38, 63, 8, 4, 126, 127, 128, 8, 12, 129, 23, 130, 3],\n",
       " [125, 38, 63, 8, 4, 126, 127, 128, 8, 12, 129, 23, 130, 3, 64],\n",
       " [125, 38, 63, 8, 4, 126, 127, 128, 8, 12, 129, 23, 130, 3, 64, 2],\n",
       " [125, 38, 63, 8, 4, 126, 127, 128, 8, 12, 129, 23, 130, 3, 64, 2, 65],\n",
       " [66, 6],\n",
       " [66, 6, 131],\n",
       " [66, 6, 131, 132],\n",
       " [66, 6, 131, 132, 39],\n",
       " [66, 6, 131, 132, 39, 133],\n",
       " [66, 6, 131, 132, 39, 133, 67],\n",
       " [66, 6, 131, 132, 39, 133, 67, 134],\n",
       " [66, 6, 131, 132, 39, 133, 67, 134, 68],\n",
       " [66, 6, 131, 132, 39, 133, 67, 134, 68, 69],\n",
       " [66, 6, 131, 132, 39, 133, 67, 134, 68, 69, 10],\n",
       " [66, 6, 131, 132, 39, 133, 67, 134, 68, 69, 10, 70],\n",
       " [66, 6, 131, 132, 39, 133, 67, 134, 68, 69, 10, 70, 135],\n",
       " [66, 6, 131, 132, 39, 133, 67, 134, 68, 69, 10, 70, 135, 3],\n",
       " [66, 6, 131, 132, 39, 133, 67, 134, 68, 69, 10, 70, 135, 3, 136],\n",
       " [66, 6, 131, 132, 39, 133, 67, 134, 68, 69, 10, 70, 135, 3, 136, 137],\n",
       " [66, 6, 131, 132, 39, 133, 67, 134, 68, 69, 10, 70, 135, 3, 136, 137, 71],\n",
       " [66, 6, 131, 132, 39, 133, 67, 134, 68, 69, 10, 70, 135, 3, 136, 137, 71, 1],\n",
       " [66,\n",
       "  6,\n",
       "  131,\n",
       "  132,\n",
       "  39,\n",
       "  133,\n",
       "  67,\n",
       "  134,\n",
       "  68,\n",
       "  69,\n",
       "  10,\n",
       "  70,\n",
       "  135,\n",
       "  3,\n",
       "  136,\n",
       "  137,\n",
       "  71,\n",
       "  1,\n",
       "  36],\n",
       " [72, 73],\n",
       " [72, 73, 74],\n",
       " [72, 73, 74, 10],\n",
       " [72, 73, 74, 10, 9],\n",
       " [72, 73, 74, 10, 9, 40],\n",
       " [72, 73, 74, 10, 9, 40, 41],\n",
       " [72, 73, 74, 10, 9, 40, 41, 75],\n",
       " [72, 73, 74, 10, 9, 40, 41, 75, 5],\n",
       " [72, 73, 74, 10, 9, 40, 41, 75, 5, 31],\n",
       " [72, 73, 74, 10, 9, 40, 41, 75, 5, 31, 138],\n",
       " [72, 73, 74, 10, 9, 40, 41, 75, 5, 31, 138, 24],\n",
       " [72, 73, 74, 10, 9, 40, 41, 75, 5, 31, 138, 24, 139],\n",
       " [72, 73, 74, 10, 9, 40, 41, 75, 5, 31, 138, 24, 139, 1],\n",
       " [72, 73, 74, 10, 9, 40, 41, 75, 5, 31, 138, 24, 139, 1, 36],\n",
       " [72, 73, 74, 10, 9, 40, 41, 75, 5, 31, 138, 24, 139, 1, 36, 140],\n",
       " [141, 42],\n",
       " [141, 42, 8],\n",
       " [141, 42, 8, 25],\n",
       " [141, 42, 8, 25, 26],\n",
       " [141, 42, 8, 25, 26, 142],\n",
       " [141, 42, 8, 25, 26, 142, 27],\n",
       " [141, 42, 8, 25, 26, 142, 27, 143],\n",
       " [141, 42, 8, 25, 26, 142, 27, 143, 28],\n",
       " [141, 42, 8, 25, 26, 142, 27, 143, 28, 144],\n",
       " [141, 42, 8, 25, 26, 142, 27, 143, 28, 144, 27],\n",
       " [141, 42, 8, 25, 26, 142, 27, 143, 28, 144, 27, 145],\n",
       " [141, 42, 8, 25, 26, 142, 27, 143, 28, 144, 27, 145, 34],\n",
       " [141, 42, 8, 25, 26, 142, 27, 143, 28, 144, 27, 145, 34, 146],\n",
       " [141, 42, 8, 25, 26, 142, 27, 143, 28, 144, 27, 145, 34, 146, 147],\n",
       " [148, 149],\n",
       " [148, 149, 3],\n",
       " [148, 149, 3, 150],\n",
       " [148, 149, 3, 150, 151],\n",
       " [148, 149, 3, 150, 151, 152],\n",
       " [148, 149, 3, 150, 151, 152, 6],\n",
       " [148, 149, 3, 150, 151, 152, 6, 12],\n",
       " [148, 149, 3, 150, 151, 152, 6, 12, 153],\n",
       " [148, 149, 3, 150, 151, 152, 6, 12, 153, 3],\n",
       " [148, 149, 3, 150, 151, 152, 6, 12, 153, 3, 154],\n",
       " [1, 15],\n",
       " [1, 15, 155],\n",
       " [1, 15, 155, 156],\n",
       " [1, 15, 155, 156, 157],\n",
       " [1, 15, 155, 156, 157, 158],\n",
       " [1, 15, 155, 156, 157, 158, 32],\n",
       " [1, 15, 155, 156, 157, 158, 32, 1],\n",
       " [1, 15, 155, 156, 157, 158, 32, 1, 76],\n",
       " [1, 15, 155, 156, 157, 158, 32, 1, 76, 159],\n",
       " [1, 15, 155, 156, 157, 158, 32, 1, 76, 159, 19],\n",
       " [1, 15, 155, 156, 157, 158, 32, 1, 76, 159, 19, 77],\n",
       " [1, 15, 155, 156, 157, 158, 32, 1, 76, 159, 19, 77, 70],\n",
       " [1, 15, 155, 156, 157, 158, 32, 1, 76, 159, 19, 77, 70, 160],\n",
       " [1, 15, 155, 156, 157, 158, 32, 1, 76, 159, 19, 77, 70, 160, 161],\n",
       " [1, 15, 155, 156, 157, 158, 32, 1, 76, 159, 19, 77, 70, 160, 161, 33],\n",
       " [1, 15, 155, 156, 157, 158, 32, 1, 76, 159, 19, 77, 70, 160, 161, 33, 9],\n",
       " [1, 15, 155, 156, 157, 158, 32, 1, 76, 159, 19, 77, 70, 160, 161, 33, 9, 162],\n",
       " [1,\n",
       "  15,\n",
       "  155,\n",
       "  156,\n",
       "  157,\n",
       "  158,\n",
       "  32,\n",
       "  1,\n",
       "  76,\n",
       "  159,\n",
       "  19,\n",
       "  77,\n",
       "  70,\n",
       "  160,\n",
       "  161,\n",
       "  33,\n",
       "  9,\n",
       "  162,\n",
       "  4],\n",
       " [1,\n",
       "  15,\n",
       "  155,\n",
       "  156,\n",
       "  157,\n",
       "  158,\n",
       "  32,\n",
       "  1,\n",
       "  76,\n",
       "  159,\n",
       "  19,\n",
       "  77,\n",
       "  70,\n",
       "  160,\n",
       "  161,\n",
       "  33,\n",
       "  9,\n",
       "  162,\n",
       "  4,\n",
       "  163],\n",
       " [43, 3],\n",
       " [43, 3, 78],\n",
       " [43, 3, 78, 16],\n",
       " [43, 3, 78, 16, 44],\n",
       " [43, 3, 78, 16, 44, 79],\n",
       " [43, 3, 78, 16, 44, 79, 2],\n",
       " [43, 3, 78, 16, 44, 79, 2, 164],\n",
       " [43, 3, 78, 16, 44, 79, 2, 164, 1],\n",
       " [43, 3, 78, 16, 44, 79, 2, 164, 1, 37],\n",
       " [43, 3, 78, 16, 44, 79, 2, 164, 1, 37, 21],\n",
       " [165, 6],\n",
       " [165, 6, 12],\n",
       " [165, 6, 12, 166],\n",
       " [165, 6, 12, 166, 167],\n",
       " [165, 6, 12, 166, 167, 23],\n",
       " [165, 6, 12, 166, 167, 23, 18],\n",
       " [165, 6, 12, 166, 167, 23, 18, 168],\n",
       " [165, 6, 12, 166, 167, 23, 18, 168, 169],\n",
       " [165, 6, 12, 166, 167, 23, 18, 168, 169, 27],\n",
       " [165, 6, 12, 166, 167, 23, 18, 168, 169, 27, 2],\n",
       " [165, 6, 12, 166, 167, 23, 18, 168, 169, 27, 2, 170],\n",
       " [165, 6, 12, 166, 167, 23, 18, 168, 169, 27, 2, 170, 171],\n",
       " [165, 6, 12, 166, 167, 23, 18, 168, 169, 27, 2, 170, 171, 13],\n",
       " [165, 6, 12, 166, 167, 23, 18, 168, 169, 27, 2, 170, 171, 13, 17],\n",
       " [165, 6, 12, 166, 167, 23, 18, 168, 169, 27, 2, 170, 171, 13, 17, 7],\n",
       " [2, 172],\n",
       " [2, 172, 1],\n",
       " [2, 172, 1, 80],\n",
       " [2, 172, 1, 80, 9],\n",
       " [2, 172, 1, 80, 9, 18],\n",
       " [2, 172, 1, 80, 9, 18, 173],\n",
       " [2, 172, 1, 80, 9, 18, 173, 81],\n",
       " [2, 172, 1, 80, 9, 18, 173, 81, 29],\n",
       " [2, 172, 1, 80, 9, 18, 173, 81, 29, 82],\n",
       " [2, 172, 1, 80, 9, 18, 173, 81, 29, 82, 25],\n",
       " [2, 172, 1, 80, 9, 18, 173, 81, 29, 82, 25, 26],\n",
       " [2, 172, 1, 80, 9, 18, 173, 81, 29, 82, 25, 26, 1],\n",
       " [2, 172, 1, 80, 9, 18, 173, 81, 29, 82, 25, 26, 1, 174],\n",
       " [2, 172, 1, 80, 9, 18, 173, 81, 29, 82, 25, 26, 1, 174, 175],\n",
       " [2, 172, 1, 80, 9, 18, 173, 81, 29, 82, 25, 26, 1, 174, 175, 6],\n",
       " [2, 172, 1, 80, 9, 18, 173, 81, 29, 82, 25, 26, 1, 174, 175, 6, 14],\n",
       " [2, 172, 1, 80, 9, 18, 173, 81, 29, 82, 25, 26, 1, 174, 175, 6, 14, 79],\n",
       " [2, 172, 1, 80, 9, 18, 173, 81, 29, 82, 25, 26, 1, 174, 175, 6, 14, 79, 32],\n",
       " [2,\n",
       "  172,\n",
       "  1,\n",
       "  80,\n",
       "  9,\n",
       "  18,\n",
       "  173,\n",
       "  81,\n",
       "  29,\n",
       "  82,\n",
       "  25,\n",
       "  26,\n",
       "  1,\n",
       "  174,\n",
       "  175,\n",
       "  6,\n",
       "  14,\n",
       "  79,\n",
       "  32,\n",
       "  42],\n",
       " [2,\n",
       "  172,\n",
       "  1,\n",
       "  80,\n",
       "  9,\n",
       "  18,\n",
       "  173,\n",
       "  81,\n",
       "  29,\n",
       "  82,\n",
       "  25,\n",
       "  26,\n",
       "  1,\n",
       "  174,\n",
       "  175,\n",
       "  6,\n",
       "  14,\n",
       "  79,\n",
       "  32,\n",
       "  42,\n",
       "  2],\n",
       " [2,\n",
       "  172,\n",
       "  1,\n",
       "  80,\n",
       "  9,\n",
       "  18,\n",
       "  173,\n",
       "  81,\n",
       "  29,\n",
       "  82,\n",
       "  25,\n",
       "  26,\n",
       "  1,\n",
       "  174,\n",
       "  175,\n",
       "  6,\n",
       "  14,\n",
       "  79,\n",
       "  32,\n",
       "  42,\n",
       "  2,\n",
       "  1],\n",
       " [2,\n",
       "  172,\n",
       "  1,\n",
       "  80,\n",
       "  9,\n",
       "  18,\n",
       "  173,\n",
       "  81,\n",
       "  29,\n",
       "  82,\n",
       "  25,\n",
       "  26,\n",
       "  1,\n",
       "  174,\n",
       "  175,\n",
       "  6,\n",
       "  14,\n",
       "  79,\n",
       "  32,\n",
       "  42,\n",
       "  2,\n",
       "  1,\n",
       "  83],\n",
       " [45, 8],\n",
       " [45, 8, 30],\n",
       " [45, 8, 30, 39],\n",
       " [45, 8, 30, 39, 29],\n",
       " [45, 8, 30, 39, 29, 53],\n",
       " [45, 8, 30, 39, 29, 53, 176],\n",
       " [45, 8, 30, 39, 29, 53, 176, 84],\n",
       " [45, 8, 30, 39, 29, 53, 176, 84, 85],\n",
       " [45, 8, 30, 39, 29, 53, 176, 84, 85, 177],\n",
       " [45, 8, 30, 39, 29, 53, 176, 84, 85, 177, 178],\n",
       " [45, 8, 30, 39, 29, 53, 176, 84, 85, 177, 178, 179],\n",
       " [180, 181],\n",
       " [180, 181, 86],\n",
       " [180, 181, 86, 3],\n",
       " [180, 181, 86, 3, 182],\n",
       " [180, 181, 86, 3, 182, 183],\n",
       " [180, 181, 86, 3, 182, 183, 7],\n",
       " [180, 181, 86, 3, 182, 183, 7, 13],\n",
       " [180, 181, 86, 3, 182, 183, 7, 13, 87],\n",
       " [6, 23],\n",
       " [6, 23, 57],\n",
       " [6, 23, 57, 40],\n",
       " [6, 23, 57, 40, 16],\n",
       " [6, 23, 57, 40, 16, 44],\n",
       " [6, 23, 57, 40, 16, 44, 184],\n",
       " [6, 23, 57, 40, 16, 44, 184, 2],\n",
       " [6, 23, 57, 40, 16, 44, 184, 2, 46],\n",
       " [6, 23, 57, 40, 16, 44, 184, 2, 46, 47],\n",
       " [6, 23, 57, 40, 16, 44, 184, 2, 46, 47, 43],\n",
       " [6, 23, 57, 40, 16, 44, 184, 2, 46, 47, 43, 19],\n",
       " [6, 23, 57, 40, 16, 44, 184, 2, 46, 47, 43, 19, 88],\n",
       " [6, 23, 57, 40, 16, 44, 184, 2, 46, 47, 43, 19, 88, 6],\n",
       " [185, 24],\n",
       " [185, 24, 62],\n",
       " [185, 24, 62, 22],\n",
       " [185, 24, 62, 22, 11],\n",
       " [185, 24, 62, 22, 11, 38],\n",
       " [185, 24, 62, 22, 11, 38, 17],\n",
       " [185, 24, 62, 22, 11, 38, 17, 7],\n",
       " [185, 24, 62, 22, 11, 38, 17, 7, 28],\n",
       " [185, 24, 62, 22, 11, 38, 17, 7, 28, 186],\n",
       " [185, 24, 62, 22, 11, 38, 17, 7, 28, 186, 187],\n",
       " [185, 24, 62, 22, 11, 38, 17, 7, 28, 186, 187, 66],\n",
       " [185, 24, 62, 22, 11, 38, 17, 7, 28, 186, 187, 66, 188],\n",
       " [13, 14],\n",
       " [13, 14, 60],\n",
       " [13, 14, 60, 20],\n",
       " [13, 14, 60, 20, 189],\n",
       " [13, 14, 60, 20, 189, 190],\n",
       " [13, 14, 60, 20, 189, 190, 19],\n",
       " [13, 14, 60, 20, 189, 190, 19, 191],\n",
       " [13, 14, 60, 20, 189, 190, 19, 191, 5],\n",
       " [13, 14, 60, 20, 189, 190, 19, 191, 5, 39],\n",
       " [13, 14, 60, 20, 189, 190, 19, 191, 5, 39, 192],\n",
       " [13, 14, 60, 20, 189, 190, 19, 191, 5, 39, 192, 7],\n",
       " [86, 8],\n",
       " [86, 8, 17],\n",
       " [86, 8, 17, 7],\n",
       " [86, 8, 17, 7, 48],\n",
       " [86, 8, 17, 7, 48, 1],\n",
       " [86, 8, 17, 7, 48, 1, 15],\n",
       " [86, 8, 17, 7, 48, 1, 15, 193],\n",
       " [86, 8, 17, 7, 48, 1, 15, 193, 194],\n",
       " [86, 8, 17, 7, 48, 1, 15, 193, 194, 89],\n",
       " [86, 8, 17, 7, 48, 1, 15, 193, 194, 89, 11],\n",
       " [86, 8, 17, 7, 48, 1, 15, 193, 194, 89, 11, 1],\n",
       " [86, 8, 17, 7, 48, 1, 15, 193, 194, 89, 11, 1, 90],\n",
       " [86, 8, 17, 7, 48, 1, 15, 193, 194, 89, 11, 1, 90, 4],\n",
       " [86, 8, 17, 7, 48, 1, 15, 193, 194, 89, 11, 1, 90, 4, 1],\n",
       " [86, 8, 17, 7, 48, 1, 15, 193, 194, 89, 11, 1, 90, 4, 1, 13],\n",
       " [8, 12],\n",
       " [8, 12, 195],\n",
       " [8, 12, 195, 20],\n",
       " [8, 12, 195, 20, 1],\n",
       " [8, 12, 195, 20, 1, 83],\n",
       " [8, 12, 195, 20, 1, 83, 91],\n",
       " [8, 12, 195, 20, 1, 83, 91, 28],\n",
       " [8, 12, 195, 20, 1, 83, 91, 28, 91],\n",
       " [8, 12, 195, 20, 1, 83, 91, 28, 91, 196],\n",
       " [8, 12, 195, 20, 1, 83, 91, 28, 91, 196, 197],\n",
       " [8, 12, 195, 20, 1, 83, 91, 28, 91, 196, 197, 198],\n",
       " [8, 12, 195, 20, 1, 83, 91, 28, 91, 196, 197, 198, 199],\n",
       " [8, 12, 195, 20, 1, 83, 91, 28, 91, 196, 197, 198, 199, 6],\n",
       " [8, 45],\n",
       " [8, 45, 67],\n",
       " [8, 45, 67, 18],\n",
       " [8, 45, 67, 18, 200],\n",
       " [8, 45, 67, 18, 200, 201],\n",
       " [8, 45, 67, 18, 200, 201, 12],\n",
       " [8, 45, 67, 18, 200, 201, 12, 64],\n",
       " [8, 45, 67, 18, 200, 201, 12, 64, 2],\n",
       " [8, 45, 67, 18, 200, 201, 12, 64, 2, 65],\n",
       " [8, 45, 67, 18, 200, 201, 12, 64, 2, 65, 202],\n",
       " [8, 45, 67, 18, 200, 201, 12, 64, 2, 65, 202, 203],\n",
       " [69, 10],\n",
       " [69, 10, 204],\n",
       " [69, 10, 204, 205],\n",
       " [69, 10, 204, 205, 206],\n",
       " [69, 10, 204, 205, 206, 3],\n",
       " [69, 10, 204, 205, 206, 3, 207],\n",
       " [69, 10, 204, 205, 206, 3, 207, 4],\n",
       " [69, 10, 204, 205, 206, 3, 207, 4, 6],\n",
       " [69, 10, 204, 205, 206, 3, 207, 4, 6, 208],\n",
       " [69, 10, 204, 205, 206, 3, 207, 4, 6, 208, 209],\n",
       " [92, 1],\n",
       " [92, 1, 82],\n",
       " [92, 1, 82, 81],\n",
       " [92, 1, 82, 81, 210],\n",
       " [92, 1, 82, 81, 210, 211],\n",
       " [92, 1, 82, 81, 210, 211, 212],\n",
       " [92, 1, 82, 81, 210, 211, 212, 1],\n",
       " [92, 1, 82, 81, 210, 211, 212, 1, 213],\n",
       " [92, 1, 82, 81, 210, 211, 212, 1, 213, 4],\n",
       " [92, 1, 82, 81, 210, 211, 212, 1, 213, 4, 29],\n",
       " [92, 1, 82, 81, 210, 211, 212, 1, 213, 4, 29, 214],\n",
       " [92, 1, 82, 81, 210, 211, 212, 1, 213, 4, 29, 214, 28],\n",
       " [92, 1, 82, 81, 210, 211, 212, 1, 213, 4, 29, 214, 28, 1],\n",
       " [92, 1, 82, 81, 210, 211, 212, 1, 213, 4, 29, 214, 28, 1, 15],\n",
       " [68, 215],\n",
       " [68, 215, 10],\n",
       " [68, 215, 10, 216],\n",
       " [68, 215, 10, 216, 3],\n",
       " [68, 215, 10, 216, 3, 217],\n",
       " [68, 215, 10, 216, 3, 217, 24],\n",
       " [68, 215, 10, 216, 3, 217, 24, 1],\n",
       " [68, 215, 10, 216, 3, 217, 24, 1, 15],\n",
       " [68, 215, 10, 216, 3, 217, 24, 1, 15, 218],\n",
       " [68, 215, 10, 216, 3, 217, 24, 1, 15, 218, 23],\n",
       " [68, 215, 10, 216, 3, 217, 24, 1, 15, 218, 23, 58],\n",
       " [68, 215, 10, 216, 3, 217, 24, 1, 15, 218, 23, 58, 219],\n",
       " [220, 10],\n",
       " [220, 10, 221],\n",
       " [220, 10, 221, 3],\n",
       " [220, 10, 221, 3, 222],\n",
       " [220, 10, 221, 3, 222, 223],\n",
       " [220, 10, 221, 3, 222, 223, 224],\n",
       " [220, 10, 221, 3, 222, 223, 224, 225],\n",
       " [220, 10, 221, 3, 222, 223, 224, 225, 17],\n",
       " [220, 10, 221, 3, 222, 223, 224, 225, 17, 93],\n",
       " [226, 227],\n",
       " [226, 227, 4],\n",
       " [226, 227, 4, 27],\n",
       " [226, 227, 4, 27, 228],\n",
       " [226, 227, 4, 27, 228, 16],\n",
       " [226, 227, 4, 27, 228, 16, 229],\n",
       " [226, 227, 4, 27, 228, 16, 229, 31],\n",
       " [226, 227, 4, 27, 228, 16, 229, 31, 230],\n",
       " [88, 9],\n",
       " [88, 9, 231],\n",
       " [88, 9, 231, 12],\n",
       " [88, 9, 231, 12, 232],\n",
       " [88, 9, 231, 12, 232, 233],\n",
       " [88, 9, 231, 12, 232, 233, 2],\n",
       " [88, 9, 231, 12, 232, 233, 2, 234],\n",
       " [88, 9, 231, 12, 232, 233, 2, 234, 235],\n",
       " [88, 9, 231, 12, 232, 233, 2, 234, 235, 236],\n",
       " [72, 9],\n",
       " [72, 9, 237],\n",
       " [72, 9, 237, 24],\n",
       " [72, 9, 237, 24, 238],\n",
       " [72, 9, 237, 24, 238, 5],\n",
       " [72, 9, 237, 24, 238, 5, 239],\n",
       " [72, 9, 237, 24, 238, 5, 239, 240],\n",
       " [72, 9, 237, 24, 238, 5, 239, 240, 241],\n",
       " [242, 8],\n",
       " [242, 8, 243],\n",
       " [242, 8, 243, 244],\n",
       " [242, 8, 243, 244, 9],\n",
       " [242, 8, 243, 244, 9, 14],\n",
       " [242, 8, 243, 244, 9, 14, 245],\n",
       " [242, 8, 243, 244, 9, 14, 245, 4],\n",
       " [242, 8, 243, 244, 9, 14, 245, 4, 246],\n",
       " [242, 8, 243, 244, 9, 14, 245, 4, 246, 7],\n",
       " [242, 8, 243, 244, 9, 14, 245, 4, 246, 7, 8],\n",
       " [1, 247],\n",
       " [1, 247, 4],\n",
       " [1, 247, 4, 54],\n",
       " [1, 247, 4, 54, 248],\n",
       " [1, 247, 4, 54, 248, 25],\n",
       " [1, 247, 4, 54, 248, 25, 26],\n",
       " [1, 247, 4, 54, 248, 25, 26, 249],\n",
       " [1, 247, 4, 54, 248, 25, 26, 249, 250],\n",
       " [1, 247, 4, 54, 248, 25, 26, 249, 250, 71],\n",
       " [1, 247, 4, 54, 248, 25, 26, 249, 250, 71, 251],\n",
       " [1, 247, 4, 54, 248, 25, 26, 249, 250, 71, 251, 5],\n",
       " [1, 247, 4, 54, 248, 25, 26, 249, 250, 71, 251, 5, 84],\n",
       " [1, 247, 4, 54, 248, 25, 26, 249, 250, 71, 251, 5, 84, 85],\n",
       " [252, 55],\n",
       " [252, 55, 30],\n",
       " [252, 55, 30, 253],\n",
       " [252, 55, 30, 253, 11],\n",
       " [252, 55, 30, 253, 11, 8],\n",
       " [252, 55, 30, 253, 11, 8, 94],\n",
       " [252, 55, 30, 253, 11, 8, 94, 254],\n",
       " [252, 55, 30, 253, 11, 8, 94, 254, 19],\n",
       " [252, 55, 30, 253, 11, 8, 94, 254, 19, 45],\n",
       " [252, 55, 30, 253, 11, 8, 94, 254, 19, 45, 255],\n",
       " [48, 256],\n",
       " [48, 256, 257],\n",
       " [48, 256, 257, 17],\n",
       " [48, 256, 257, 17, 93],\n",
       " [7, 14],\n",
       " [7, 14, 258],\n",
       " [7, 14, 258, 259],\n",
       " [7, 14, 258, 259, 61],\n",
       " [7, 14, 258, 259, 61, 76],\n",
       " [7, 14, 258, 259, 61, 76, 260],\n",
       " [7, 14, 258, 259, 61, 76, 260, 94],\n",
       " [7, 14, 258, 259, 61, 76, 260, 94, 261],\n",
       " [7, 14, 258, 259, 61, 76, 260, 94, 261, 22],\n",
       " [262, 34],\n",
       " [262, 34, 1],\n",
       " [262, 34, 1, 263],\n",
       " [262, 34, 1, 263, 264],\n",
       " [262, 34, 1, 263, 264, 40],\n",
       " [262, 34, 1, 263, 264, 40, 265],\n",
       " [262, 34, 1, 263, 264, 40, 265, 266],\n",
       " [262, 34, 1, 263, 264, 40, 265, 266, 267],\n",
       " [262, 34, 1, 263, 264, 40, 265, 266, 267, 11],\n",
       " [262, 34, 1, 263, 264, 40, 265, 266, 267, 11, 1],\n",
       " [262, 34, 1, 263, 264, 40, 265, 266, 267, 11, 1, 73],\n",
       " [92, 95],\n",
       " [92, 95, 268],\n",
       " [92, 95, 268, 17],\n",
       " [92, 95, 268, 17, 7],\n",
       " [92, 95, 268, 17, 7, 35],\n",
       " [92, 95, 268, 17, 7, 35, 269],\n",
       " [270, 271],\n",
       " [270, 271, 63],\n",
       " [270, 271, 63, 272],\n",
       " [270, 271, 63, 272, 59],\n",
       " [270, 271, 63, 272, 59, 25],\n",
       " [270, 271, 63, 272, 59, 25, 26],\n",
       " [270, 271, 63, 272, 59, 25, 26, 273],\n",
       " [270, 271, 63, 272, 59, 25, 26, 273, 3],\n",
       " [270, 271, 63, 272, 59, 25, 26, 273, 3, 274],\n",
       " [275, 276],\n",
       " [275, 276, 277],\n",
       " [275, 276, 277, 16],\n",
       " [275, 276, 277, 16, 96],\n",
       " [275, 276, 277, 16, 96, 1],\n",
       " [275, 276, 277, 16, 96, 1, 37],\n",
       " [275, 276, 277, 16, 96, 1, 37, 278],\n",
       " [275, 276, 277, 16, 96, 1, 37, 278, 2],\n",
       " [275, 276, 277, 16, 96, 1, 37, 278, 2, 30],\n",
       " [275, 276, 277, 16, 96, 1, 37, 278, 2, 30, 90],\n",
       " [77, 279],\n",
       " [77, 279, 38],\n",
       " [77, 279, 38, 30],\n",
       " [77, 279, 38, 30, 280],\n",
       " [77, 279, 38, 30, 280, 281],\n",
       " [77, 279, 38, 30, 280, 281, 5],\n",
       " [77, 279, 38, 30, 280, 281, 5, 282],\n",
       " [77, 279, 38, 30, 280, 281, 5, 282, 29],\n",
       " [97, 283],\n",
       " [97, 283, 2],\n",
       " [97, 283, 2, 42],\n",
       " [97, 283, 2, 42, 13],\n",
       " [97, 283, 2, 42, 13, 48],\n",
       " [97, 283, 2, 42, 13, 48, 22],\n",
       " [97, 283, 2, 42, 13, 48, 22, 5],\n",
       " [97, 283, 2, 42, 13, 48, 22, 5, 35],\n",
       " [97, 283, 2, 42, 13, 48, 22, 5, 35, 284],\n",
       " [285, 286],\n",
       " [285, 286, 11],\n",
       " [285, 286, 11, 287],\n",
       " [285, 286, 11, 287, 288],\n",
       " [285, 286, 11, 287, 288, 16],\n",
       " [285, 286, 11, 287, 288, 16, 44],\n",
       " [285, 286, 11, 287, 288, 16, 44, 289],\n",
       " [285, 286, 11, 287, 288, 16, 44, 289, 19],\n",
       " [285, 286, 11, 287, 288, 16, 44, 289, 19, 41],\n",
       " [285, 286, 11, 287, 288, 16, 44, 289, 19, 41, 87],\n",
       " [290, 74],\n",
       " [290, 74, 10],\n",
       " [290, 74, 10, 291],\n",
       " [290, 74, 10, 291, 292],\n",
       " [290, 74, 10, 291, 292, 5],\n",
       " [290, 74, 10, 291, 292, 5, 293],\n",
       " [290, 74, 10, 291, 292, 5, 293, 4],\n",
       " [290, 74, 10, 291, 292, 5, 293, 4, 41],\n",
       " [290, 74, 10, 291, 292, 5, 293, 4, 41, 75],\n",
       " [97, 294],\n",
       " [97, 294, 10],\n",
       " [97, 294, 10, 295],\n",
       " [97, 294, 10, 295, 296],\n",
       " [97, 294, 10, 295, 296, 3],\n",
       " [97, 294, 10, 295, 296, 3, 297],\n",
       " [97, 294, 10, 295, 296, 3, 297, 298],\n",
       " [97, 294, 10, 295, 296, 3, 297, 298, 16],\n",
       " [97, 294, 10, 295, 296, 3, 297, 298, 16, 96],\n",
       " [97, 294, 10, 295, 296, 3, 297, 298, 16, 96, 15],\n",
       " [97, 294, 10, 295, 296, 3, 297, 298, 16, 96, 15, 21],\n",
       " [299, 1],\n",
       " [299, 1, 15],\n",
       " [299, 1, 15, 49],\n",
       " [299, 1, 15, 49, 300],\n",
       " [299, 1, 15, 49, 300, 301],\n",
       " [299, 1, 15, 49, 300, 301, 10],\n",
       " [299, 1, 15, 49, 300, 301, 10, 302],\n",
       " [299, 1, 15, 49, 300, 301, 10, 302, 9],\n",
       " [299, 1, 15, 49, 300, 301, 10, 302, 9, 47],\n",
       " [299, 1, 15, 49, 300, 301, 10, 302, 9, 47, 2],\n",
       " [299, 1, 15, 49, 300, 301, 10, 302, 9, 47, 2, 46],\n",
       " [299, 1, 15, 49, 300, 301, 10, 302, 9, 47, 2, 46, 43],\n",
       " [303, 10],\n",
       " [303, 10, 78],\n",
       " [303, 10, 78, 304],\n",
       " [303, 10, 78, 304, 3],\n",
       " [303, 10, 78, 304, 3, 305],\n",
       " [303, 10, 78, 304, 3, 305, 306],\n",
       " [303, 10, 78, 304, 3, 305, 306, 80],\n",
       " [303, 10, 78, 304, 3, 305, 306, 80, 9],\n",
       " [303, 10, 78, 304, 3, 305, 306, 80, 9, 47],\n",
       " [303, 10, 78, 304, 3, 305, 306, 80, 9, 47, 21],\n",
       " [307, 308],\n",
       " [307, 308, 49],\n",
       " [307, 308, 49, 309],\n",
       " [307, 308, 49, 309, 4],\n",
       " [307, 308, 49, 309, 4, 13],\n",
       " [307, 308, 49, 309, 4, 13, 2],\n",
       " [307, 308, 49, 309, 4, 13, 2, 46],\n",
       " [307, 308, 49, 309, 4, 13, 2, 46, 95],\n",
       " [307, 308, 49, 309, 4, 13, 2, 46, 95, 49],\n",
       " [307, 308, 49, 309, 4, 13, 2, 46, 95, 49, 310],\n",
       " [307, 308, 49, 309, 4, 13, 2, 46, 95, 49, 310, 89]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "CrzbvUUQCXPU"
   },
   "outputs": [],
   "source": [
    "max_len = max([len(x) for x in input_sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "9oPMoWBSD1_U"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "padded_input_sequences = pad_sequences(input_sequences, maxlen = max_len, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "miRb-QZyIi7_",
    "outputId": "de5c2b4e-b6e6-4e4b-a892-f97eff72c1c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,   0,  98,   9],\n",
       "       [  0,   0,   0, ...,  98,   9,  11],\n",
       "       [  0,   0,   0, ...,   9,  11,   1],\n",
       "       ...,\n",
       "       [  0,   0,   0, ...,  46,  95,  49],\n",
       "       [  0,   0,   0, ...,  95,  49, 310],\n",
       "       [  0,   0,   0, ...,  49, 310,  89]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "qVI0-UUrIsd3"
   },
   "outputs": [],
   "source": [
    "X = padded_input_sequences[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "lXrYHTDFI3uE"
   },
   "outputs": [],
   "source": [
    "y = padded_input_sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,   0,   0,  98],\n",
       "       [  0,   0,   0, ...,   0,  98,   9],\n",
       "       [  0,   0,   0, ...,  98,   9,  11],\n",
       "       ...,\n",
       "       [  0,   0,   0, ...,   2,  46,  95],\n",
       "       [  0,   0,   0, ...,  46,  95,  49],\n",
       "       [  0,   0,   0, ...,  95,  49, 310]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kmsFnHx1Qdow",
    "outputId": "38cc3392-f099-49e2-f0c9-52955380d43e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(541, 23)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-wyYqYgZSeck",
    "outputId": "5c223cc0-a8e9-4c4f-a364-a436bc492abf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(541,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "5OL3vrEXSs_s"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "rs1NPitwSgzk"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y = to_categorical(y,num_classes=311)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iQMJ0I6xSiZf",
    "outputId": "bba7b827-0696-427e-9d3d-c811600ce5a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(541, 311)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "9kVeTvR2S8Fk"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "wo-OYfHpTK2o"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(311, 20, input_length=23))\n",
    "\n",
    "model.add(LSTM(200))\n",
    "\n",
    "model.add(Dense(311, activation='sigmoid'))\n",
    "model.build(input_shape=(None, 23))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OxxXkrSXfIBv",
    "outputId": "d809aa2b-600f-4549-cfc1-1dfd1fef1a27"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">6,220</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">176,800</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">311</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">62,511</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m20\u001b[0m)              │           \u001b[38;5;34m6,220\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)                 │         \u001b[38;5;34m176,800\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m311\u001b[0m)                 │          \u001b[38;5;34m62,511\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">245,531</span> (959.11 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m245,531\u001b[0m (959.11 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">245,531</span> (959.11 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m245,531\u001b[0m (959.11 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "-GGjqh7ue_Yq"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LpFUCALCfJRR",
    "outputId": "96d67a78-3c2e-4462-b2a8-2655c303af8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.0194 - loss: 5.7340\n",
      "Epoch 2/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.0541 - loss: 5.4867\n",
      "Epoch 3/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.0278 - loss: 5.4106\n",
      "Epoch 4/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.0268 - loss: 5.3939\n",
      "Epoch 5/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.0416 - loss: 5.2899\n",
      "Epoch 6/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.0486 - loss: 5.2641\n",
      "Epoch 7/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.0551 - loss: 5.2254\n",
      "Epoch 8/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.0420 - loss: 5.2575\n",
      "Epoch 9/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.0435 - loss: 5.2347\n",
      "Epoch 10/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.0421 - loss: 5.1931\n",
      "Epoch 11/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.0517 - loss: 5.0826\n",
      "Epoch 12/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.0411 - loss: 5.0330\n",
      "Epoch 13/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.0558 - loss: 4.9634\n",
      "Epoch 14/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.0434 - loss: 4.9247\n",
      "Epoch 15/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.0370 - loss: 4.8510\n",
      "Epoch 16/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.0519 - loss: 4.7313\n",
      "Epoch 17/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.0806 - loss: 4.6465\n",
      "Epoch 18/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.0460 - loss: 4.6067\n",
      "Epoch 19/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.0542 - loss: 4.5387\n",
      "Epoch 20/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.0667 - loss: 4.4044\n",
      "Epoch 21/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.0812 - loss: 4.3683\n",
      "Epoch 22/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.0833 - loss: 4.2589\n",
      "Epoch 23/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.0827 - loss: 4.2422\n",
      "Epoch 24/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.0787 - loss: 4.1124\n",
      "Epoch 25/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.1418 - loss: 3.9594\n",
      "Epoch 26/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.1271 - loss: 3.8764\n",
      "Epoch 27/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.1407 - loss: 3.8109\n",
      "Epoch 28/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.1511 - loss: 3.6504\n",
      "Epoch 29/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.1771 - loss: 3.5688\n",
      "Epoch 30/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.2218 - loss: 3.4181\n",
      "Epoch 31/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.1938 - loss: 3.3511\n",
      "Epoch 32/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.2814 - loss: 3.1594\n",
      "Epoch 33/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.3227 - loss: 3.0835\n",
      "Epoch 34/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.3515 - loss: 2.9753\n",
      "Epoch 35/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.4040 - loss: 2.8111\n",
      "Epoch 36/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.5053 - loss: 2.6098\n",
      "Epoch 37/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.5254 - loss: 2.5095\n",
      "Epoch 38/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.5274 - loss: 2.4109\n",
      "Epoch 39/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.5927 - loss: 2.3070\n",
      "Epoch 40/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.5983 - loss: 2.1703\n",
      "Epoch 41/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.6266 - loss: 2.1157\n",
      "Epoch 42/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.6935 - loss: 1.9039\n",
      "Epoch 43/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.7252 - loss: 1.8148\n",
      "Epoch 44/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.7141 - loss: 1.7881\n",
      "Epoch 45/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.7685 - loss: 1.6227\n",
      "Epoch 46/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.7608 - loss: 1.5590\n",
      "Epoch 47/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.8139 - loss: 1.4323\n",
      "Epoch 48/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.8152 - loss: 1.3703\n",
      "Epoch 49/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.8355 - loss: 1.2867\n",
      "Epoch 50/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.8511 - loss: 1.2712\n",
      "Epoch 51/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.8538 - loss: 1.1459\n",
      "Epoch 52/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.8824 - loss: 1.0775\n",
      "Epoch 53/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.9043 - loss: 1.0138\n",
      "Epoch 54/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.8864 - loss: 1.0343\n",
      "Epoch 55/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9134 - loss: 0.9181\n",
      "Epoch 56/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9275 - loss: 0.8828\n",
      "Epoch 57/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9201 - loss: 0.8248\n",
      "Epoch 58/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9262 - loss: 0.7867\n",
      "Epoch 59/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9307 - loss: 0.7615\n",
      "Epoch 60/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.9344 - loss: 0.7066\n",
      "Epoch 61/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.9454 - loss: 0.6488\n",
      "Epoch 62/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.9517 - loss: 0.6582\n",
      "Epoch 63/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.9573 - loss: 0.5960\n",
      "Epoch 64/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9550 - loss: 0.5660\n",
      "Epoch 65/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9640 - loss: 0.5729\n",
      "Epoch 66/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.9686 - loss: 0.5215\n",
      "Epoch 67/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.9575 - loss: 0.5053\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9637 - loss: 0.5016\n",
      "Epoch 69/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9682 - loss: 0.4484\n",
      "Epoch 70/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9739 - loss: 0.4263\n",
      "Epoch 71/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9855 - loss: 0.3777\n",
      "Epoch 72/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.9851 - loss: 0.3839\n",
      "Epoch 73/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.9815 - loss: 0.3439\n",
      "Epoch 74/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9833 - loss: 0.3564\n",
      "Epoch 75/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9858 - loss: 0.3355\n",
      "Epoch 76/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9831 - loss: 0.3145\n",
      "Epoch 77/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9797 - loss: 0.2986\n",
      "Epoch 78/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.9853 - loss: 0.2823\n",
      "Epoch 79/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9808 - loss: 0.2910\n",
      "Epoch 80/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9675 - loss: 0.2930\n",
      "Epoch 81/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9853 - loss: 0.2619\n",
      "Epoch 82/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9878 - loss: 0.2367\n",
      "Epoch 83/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9840 - loss: 0.2574\n",
      "Epoch 84/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9874 - loss: 0.2375\n",
      "Epoch 85/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.9893 - loss: 0.2139\n",
      "Epoch 86/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.9833 - loss: 0.2279\n",
      "Epoch 87/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9856 - loss: 0.2094\n",
      "Epoch 88/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.9845 - loss: 0.2042\n",
      "Epoch 89/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9895 - loss: 0.2095\n",
      "Epoch 90/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.9903 - loss: 0.1909\n",
      "Epoch 91/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.9807 - loss: 0.1835\n",
      "Epoch 92/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.9820 - loss: 0.1858\n",
      "Epoch 93/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9832 - loss: 0.1748\n",
      "Epoch 94/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.9871 - loss: 0.1753\n",
      "Epoch 95/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9901 - loss: 0.1702\n",
      "Epoch 96/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.9852 - loss: 0.1612\n",
      "Epoch 97/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9734 - loss: 0.1597\n",
      "Epoch 98/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.9909 - loss: 0.1378\n",
      "Epoch 99/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.9909 - loss: 0.1459\n",
      "Epoch 100/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.9847 - loss: 0.1379\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x210d702ac90>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'e g'\n",
    "token_text = tokenizer.texts_to_sequences([text])[0]\n",
    "padded_token_text = pad_sequences([token_text], maxlen=24, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "268"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.argmax(model.predict(padded_token_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_text = tokenizer.texts_to_sequences([text])[0]\n",
    "  # padding\n",
    "padded_token_text = pad_sequences([token_text], maxlen=24, padding='pre')\n",
    "padded_token_text\n",
    "np.argmax(model.predict(padded_token_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PGeYGwCMfTus",
    "outputId": "2d508555-b83e-470e-e7e5-1b5c10cce70b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "Techniques like\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "Techniques like random\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "Techniques like random insertion\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "Techniques like random insertion swapping\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "Techniques like random insertion swapping or\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "Techniques like random insertion swapping or deletion\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "Techniques like random insertion swapping or deletion of\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "Techniques like random insertion swapping or deletion of words\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Techniques like random insertion swapping or deletion of words introduce\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "Techniques like random insertion swapping or deletion of words introduce variability\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "text = \"Techniques\"\n",
    "\n",
    "for i in range(10):\n",
    "  # tokenize\n",
    "    token_text = tokenizer.texts_to_sequences([text])[0]\n",
    "  # padding\n",
    "    padded_token_text = pad_sequences([token_text], maxlen=24, padding='pre')\n",
    "  # predict\n",
    "    pos = np.argmax(model.predict(padded_token_text))\n",
    "\n",
    "    for word,index in tokenizer.word_index.items():\n",
    "        if index == pos:\n",
    "            text = text + \" \" + word\n",
    "            print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TTxsj-_CjbQW",
    "outputId": "7e8e0b43-d8f5-4e87-8caf-33806965ab3a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92y7gE6pj9EZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
